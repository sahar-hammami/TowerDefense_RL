# Tower Defense RL â€“ Projet Reinforcement Learning

## ðŸŽ¯ Objectif du projet
Lâ€™objectif est de concevoir et entraÃ®ner un agent dâ€™intelligence artificielle capable de jouer Ã  un jeu **Tower Defense**, en utilisant des algorithmes de **Reinforcement Learning (RL)**.

Lâ€™agent doit :
- ProtÃ©ger sa base contre des vagues dâ€™ennemis
- Construire et placer des tours de maniÃ¨re optimale
- Maximiser les rÃ©compenses liÃ©es Ã  : la survie de la base, lâ€™Ã©limination des ennemis et la gestion des ressources

---

## ðŸŽ® Description du jeu Tower Defense
Le jeu simule un environnement stratÃ©gique oÃ¹ lâ€™agent prend des dÃ©cisions sÃ©quentielles :

- La base est lâ€™objectif principal Ã  protÃ©ger
- Les ennemis avancent le long dâ€™un chemin vers la base
- Les tours tirent automatiquement sur les ennemis dans leur portÃ©e
- Le joueur/agent reÃ§oit des ressources (gold) pour construire ou amÃ©liorer des tours
- Les vagues dâ€™ennemis deviennent progressivement plus difficiles

---

### ðŸ”¹ Ã‰tat du jeu
Chaque Ã©tape (step) est reprÃ©sentÃ©e par un vecteur dâ€™Ã©tat :
- `base_hp` : Points de vie de la base
- `gold` : Ressources disponibles
- `towers` : Nombre et positions des tours
- `enemies` : Position et vitesse des ennemis
- `step_count` : Nombre de steps Ã©coulÃ©s

---

### ðŸ”¹ Actions possibles

| Action             | Description                          |
|-------------------|--------------------------------------|
| Construire une tour | Placer une tour sur un emplacement libre |
| Ne rien faire       | Passer le tour                        |
| AmÃ©liorer une tour  | (Option future) augmenter sa puissance |

> Les actions sont discrÃ¨tes pour PPO et stochastiques pour SAC.

---

### ðŸ”¹ RÃ©compense (Reward)
- Positif : Ã‰limination des ennemis, survie de la base
- NÃ©gatif : Base endommagÃ©e, ressources mal dÃ©pensÃ©es
- Multi-objectif : permet Ã  lâ€™agent de dÃ©cider entre attaque et dÃ©fense

---

## ðŸ§  Algorithmes RL utilisÃ©s

### 1ï¸âƒ£ PPO â€“ Proximal Policy Optimization
- **Type :** Policy Gradient (on-policy)
- **Principe :** Optimise directement la politique stochastique tout en limitant les mises Ã  jour trop grandes pour plus de stabilitÃ©
- **Avantage pour Tower Defense :** DÃ©cisions efficaces pour la construction et placement des tours

**Diagramme conceptuel PPO :**

+-------------+
| Ã‰tat s_t |
+-------------+
|
v
+-------------------+
| Policy Network |
| (Actor) |
+-------------------+
|
v
+-------------+
| Action a_t |
+-------------+
|
v
+----------------+
| Environnement |
| TowerDefense |
+----------------+
|
v
+-------------+
| Reward r_t |
+-------------+
|
v
Mise Ã  jour
Policy Network

---

### 2ï¸âƒ£ SAC â€“ Soft Actor-Critic
- **Type :** Actor-Critic off-policy
- **Principe :** Apprend un actor pour choisir lâ€™action et un critic (Q-network) pour estimer les valeurs dâ€™action
- **Avantage pour Tower Defense :** GÃ¨re bien les environnements complexes et changeants

**Diagramme conceptuel SAC :**

+-------------+
| Ã‰tat s_t |
+-------------+
|
v
+-----------------+ +-----------------+
| Actor Network |------>| Action a_t |
+-----------------+ +-----------------+
| |
v v
+-----------------+ +-----------------+
| Critic Q(s,a) |<------| Reward r_t |
+-----------------+ +-----------------+
|
v
Mise Ã  jour
Actor + Critic

---

### ðŸ”¹ DiffÃ©rences clÃ©s PPO vs SAC

| CritÃ¨re        | PPO                         | SAC                        |
|----------------|-----------------------------|---------------------------|
| Type           | Policy Gradient             | Actor-Critic off-policy    |
| Exploration    | Stochastique contrÃ´lÃ©e      | Stochastique avec entropie |
| StabilitÃ©      | TrÃ¨s stable                 | Flexible mais sensible aux hyperparamÃ¨tres |
| Avantage TD    | DÃ©cisions rapides et sÃ»res  | Exploration des stratÃ©gies complexes |
| Action         | DiscrÃ¨te                    | Stochastique (discrÃ¨te ici) |

---

## ðŸ”¹ Architecture du projet

Tower Defense RL
â”œâ”€â”€ agents/
â”‚ â”œâ”€â”€ ppo_agent.py
â”‚ â””â”€â”€ sac_agent.py
â”œâ”€â”€ env/
â”‚ â””â”€â”€ td_env.py
â”œâ”€â”€ game/
â”‚ â”œâ”€â”€ engine.py
â”‚ â”œâ”€â”€ tower.py
â”‚ â””â”€â”€ enemy.py
â”œâ”€â”€ training/
â”‚ â”œâ”€â”€ train_ppo.py
â”‚ â””â”€â”€ train_sac.py
â”œâ”€â”€ visualisation/
â”‚ â””â”€â”€ render.py
â”œâ”€â”€ ppo_model.pth
â””â”€â”€ sac_model.pth

---

## ðŸ”¹ Flux global

[ Environnement Tower Defense ]
â†‘
| Reward
|
[ Agent RL (PPO / SAC) ]
|
v
Action
|
v
[ Environnement Tower Defense ]

- La boucle continue jusquâ€™Ã  la fin de lâ€™Ã©pisode (base dÃ©truite ou nombre de steps atteint).  
- Les modÃ¨les PPO et SAC sont entraÃ®nÃ©s puis visualisÃ©s avec **Pygame**.

---

## ðŸ”¹ Visualisation

- Base : vert  
- Tours : bleu  
- Ennemis : rouge  
- Infos texte : Base HP, Gold, nombre dâ€™ennemis  
- FPS : 10 (pour synchronisation avec lâ€™entraÃ®nement)  

**Capture du jeu :**

![Capture du jeu](jeux.png)  

> Rouge = ennemis, bleu = tours, vert = base

